{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI, AsyncClient\n",
    "import asyncio\n",
    "key = ''\n",
    "url = \"https://deepseek.perfxlab.cn/v1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../prompt/role_play_kimi.txt','r') as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "with open('../Output/人设v1.0/黍.md','r') as f:\n",
    "    rolecard = f.read()\n",
    "\n",
    "message = [  \n",
    "        {\"role\":\"system\",\"content\":prompt.format(rolecard)},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().strftime('%c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  test\n",
    "client = OpenAI(\n",
    "    base_url = url,\n",
    "    api_key = key\n",
    ")\n",
    "\n",
    "def test_chat(msg, history):\n",
    "\n",
    "    history.append({\"role\": \"user\", \"content\": msg})\n",
    "    res = client.chat.completions.create(\n",
    "    model=\"DeepSeek-V3\", # 指定要使用的模型\n",
    "    messages = history, # 用户消息，包含用户的输入\n",
    "    temperature=0.7, \n",
    "    max_tokens=1024, # 限制生成的最大令牌数（单词数量）\n",
    "    n=1, # 生成的回复数量\n",
    "    presence_penalty=0.3, # 控制新话题出现的惩罚，值越高，新话题出现越少\n",
    "    frequency_penalty=0.3, # 控制重复话语的惩罚，值越高，重复率越低\n",
    "    stream=False, # 启用流式响应，允许逐块接收生成内容\n",
    ")\n",
    "    history.append({\"role\":\"assistant\",\"content\":res.choices[0].message.content})\n",
    "    print(history[-1]['content'])\n",
    "    return res, history\n",
    "\n",
    "\n",
    "\n",
    "history = message.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, history = test_chat('你好', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 一轮  batchtest\n",
    "\n",
    "aclient = AsyncClient(\n",
    "    base_url = url,\n",
    "    api_key = key\n",
    ")\n",
    "\n",
    "async def acreate(massgae):\n",
    "    res = await aclient.chat.completions.create(\n",
    "    model=\"DeepSeek-V3\",\n",
    "    messages = massgae,\n",
    "    temperature=0.9, \n",
    "    max_tokens=1024, \n",
    "    n=1, \n",
    "    presence_penalty=0.3, \n",
    "    frequency_penalty=0.5, \n",
    "    stream=False,\n",
    "    )\n",
    "    return res\n",
    "\n",
    "async def test_chat_batch(msg, history):\n",
    "    tasks = [acreate(history + [{\"role\": \"system\", \"content\": f\"## 当前时间: \\n{t}\"},{\"role\": \"user\", \"content\": m}]) for m,t in msg]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "test_call = [\n",
    "    (\"在吗？\", \"星期日 2025-3-9 12:30\"),\n",
    "    (\"你今晚有空吗？\", \"星期六 2025-3-8 14:00\"),\n",
    "    (\"什么时候才能放假啊\",\"星期四 2025-3-20 15:30\"),\n",
    "    (\"\", \"星期一 2025-3-1 1:20\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "博士，我在这里。今日阳光和煦，正适合在田间劳作。您找我可是有事相商？\n",
      "博士，今夜月色正好，我本想在田埂上散步，看那稻穗在月光下摇曳。不过既然您找我，想必是有要事相商吧？只要不是与弟弟那孩子气的新游戏有关就好。\n",
      "博士，今日正值春分时节，万物复苏之际。劳逸结合固然重要，但此刻的每一份努力，都是为将来的丰收播下希望的种子。正如农人耕耘不辍，方能期待金秋的硕果累累。让我们携手前行，待到风调雨顺、五谷丰登之日，便是我们安享闲暇之时。相信在这段时光里所付出的辛劳，终将化作美好的回忆与收获的喜悦。\n",
      "博士，夜深了，您还未歇息吗？（轻声细语地）窗外的月光洒在麦田上，如同银色的波浪，柔美而宁静。在这样的夜晚，最适合放下心中的重担，与自然对话。或许您有些心事？不妨与我分享，让我为您分担一二。\n"
     ]
    }
   ],
   "source": [
    "res = await test_chat_batch(test_call, message)\n",
    "for i in res:\n",
    "    print(i.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
